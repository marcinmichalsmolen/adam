{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNqiCg3N9LndJPoNV0XKfSg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcinmichalsmolen/adam/blob/main/TFDS_FLOWERS_VGG_ZLE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EW2ZTVJbRuzg",
        "outputId": "c0c26d6b-da12-4f8c-8cb7-28f3e040bd18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with Adagrad optimizer...\n",
            "Epoch 1/30\n",
            "46/46 [==============================] - 12s 225ms/step - loss: 1.4636 - accuracy: 0.3866 - val_loss: 1.3127 - val_accuracy: 0.5068\n",
            "Epoch 2/30\n",
            "46/46 [==============================] - 7s 150ms/step - loss: 1.2102 - accuracy: 0.5484 - val_loss: 1.1565 - val_accuracy: 0.5804\n",
            "Epoch 3/30\n",
            "46/46 [==============================] - 6s 138ms/step - loss: 1.0780 - accuracy: 0.6202 - val_loss: 1.0539 - val_accuracy: 0.6267\n",
            "Epoch 4/30\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 0.9871 - accuracy: 0.6608 - val_loss: 0.9820 - val_accuracy: 0.6540\n",
            "Epoch 5/30\n",
            "46/46 [==============================] - 7s 150ms/step - loss: 0.9197 - accuracy: 0.6945 - val_loss: 0.9295 - val_accuracy: 0.6676\n",
            "Epoch 6/30\n",
            "46/46 [==============================] - 7s 152ms/step - loss: 0.8673 - accuracy: 0.7132 - val_loss: 0.8880 - val_accuracy: 0.6757\n",
            "Epoch 7/30\n",
            "46/46 [==============================] - 8s 166ms/step - loss: 0.8255 - accuracy: 0.7323 - val_loss: 0.8548 - val_accuracy: 0.6921\n",
            "Epoch 8/30\n",
            "46/46 [==============================] - 7s 159ms/step - loss: 0.7896 - accuracy: 0.7476 - val_loss: 0.8272 - val_accuracy: 0.7084\n",
            "Epoch 9/30\n",
            "46/46 [==============================] - 7s 158ms/step - loss: 0.7593 - accuracy: 0.7568 - val_loss: 0.8038 - val_accuracy: 0.7112\n",
            "Epoch 10/30\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 0.7330 - accuracy: 0.7646 - val_loss: 0.7832 - val_accuracy: 0.7193\n",
            "Epoch 11/30\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 0.7096 - accuracy: 0.7732 - val_loss: 0.7660 - val_accuracy: 0.7248\n",
            "Epoch 12/30\n",
            "46/46 [==============================] - 7s 145ms/step - loss: 0.6886 - accuracy: 0.7813 - val_loss: 0.7508 - val_accuracy: 0.7275\n",
            "Epoch 13/30\n",
            "46/46 [==============================] - 8s 173ms/step - loss: 0.6698 - accuracy: 0.7858 - val_loss: 0.7374 - val_accuracy: 0.7302\n",
            "Epoch 14/30\n",
            "46/46 [==============================] - 8s 169ms/step - loss: 0.6526 - accuracy: 0.7919 - val_loss: 0.7257 - val_accuracy: 0.7275\n",
            "Epoch 15/30\n",
            "46/46 [==============================] - 7s 158ms/step - loss: 0.6369 - accuracy: 0.7984 - val_loss: 0.7155 - val_accuracy: 0.7302\n",
            "Epoch 16/30\n",
            "46/46 [==============================] - 8s 169ms/step - loss: 0.6224 - accuracy: 0.8025 - val_loss: 0.7063 - val_accuracy: 0.7330\n",
            "Epoch 17/30\n",
            "46/46 [==============================] - 7s 157ms/step - loss: 0.6090 - accuracy: 0.8093 - val_loss: 0.6976 - val_accuracy: 0.7357\n",
            "Epoch 18/30\n",
            "46/46 [==============================] - 7s 149ms/step - loss: 0.5966 - accuracy: 0.8137 - val_loss: 0.6899 - val_accuracy: 0.7384\n",
            "Epoch 19/30\n",
            "46/46 [==============================] - 7s 153ms/step - loss: 0.5850 - accuracy: 0.8171 - val_loss: 0.6829 - val_accuracy: 0.7439\n",
            "Epoch 20/30\n",
            "46/46 [==============================] - 8s 172ms/step - loss: 0.5740 - accuracy: 0.8215 - val_loss: 0.6763 - val_accuracy: 0.7466\n",
            "Epoch 21/30\n",
            "46/46 [==============================] - 8s 163ms/step - loss: 0.5637 - accuracy: 0.8236 - val_loss: 0.6701 - val_accuracy: 0.7466\n",
            "Epoch 22/30\n",
            "46/46 [==============================] - 8s 177ms/step - loss: 0.5539 - accuracy: 0.8266 - val_loss: 0.6646 - val_accuracy: 0.7520\n",
            "Epoch 23/30\n",
            "46/46 [==============================] - 8s 171ms/step - loss: 0.5446 - accuracy: 0.8300 - val_loss: 0.6596 - val_accuracy: 0.7520\n",
            "Epoch 24/30\n",
            "46/46 [==============================] - 7s 160ms/step - loss: 0.5357 - accuracy: 0.8317 - val_loss: 0.6551 - val_accuracy: 0.7548\n",
            "Epoch 25/30\n",
            "46/46 [==============================] - 7s 150ms/step - loss: 0.5273 - accuracy: 0.8389 - val_loss: 0.6509 - val_accuracy: 0.7548\n",
            "Epoch 26/30\n",
            "46/46 [==============================] - 8s 162ms/step - loss: 0.5194 - accuracy: 0.8386 - val_loss: 0.6467 - val_accuracy: 0.7520\n",
            "Epoch 27/30\n",
            "46/46 [==============================] - 7s 152ms/step - loss: 0.5117 - accuracy: 0.8423 - val_loss: 0.6429 - val_accuracy: 0.7548\n",
            "Epoch 28/30\n",
            "46/46 [==============================] - 8s 165ms/step - loss: 0.5044 - accuracy: 0.8454 - val_loss: 0.6394 - val_accuracy: 0.7548\n",
            "Epoch 29/30\n",
            "46/46 [==============================] - 8s 162ms/step - loss: 0.4973 - accuracy: 0.8484 - val_loss: 0.6360 - val_accuracy: 0.7602\n",
            "Epoch 30/30\n",
            "46/46 [==============================] - 7s 150ms/step - loss: 0.4905 - accuracy: 0.8508 - val_loss: 0.6330 - val_accuracy: 0.7575\n",
            "Training model with RMSprop optimizer...\n",
            "Epoch 1/30\n",
            "46/46 [==============================] - 10s 171ms/step - loss: 1.0190 - accuracy: 0.6642 - val_loss: 0.6422 - val_accuracy: 0.7548\n",
            "Epoch 2/30\n",
            "46/46 [==============================] - 8s 163ms/step - loss: 0.6333 - accuracy: 0.7725 - val_loss: 0.6483 - val_accuracy: 0.7629\n",
            "Epoch 3/30\n",
            "46/46 [==============================] - 8s 163ms/step - loss: 0.4822 - accuracy: 0.8239 - val_loss: 0.6647 - val_accuracy: 0.7575\n",
            "Epoch 4/30\n",
            "46/46 [==============================] - 8s 174ms/step - loss: 0.3942 - accuracy: 0.8644 - val_loss: 0.5934 - val_accuracy: 0.7793\n",
            "Epoch 5/30\n",
            "46/46 [==============================] - 7s 161ms/step - loss: 0.3246 - accuracy: 0.8859 - val_loss: 0.6796 - val_accuracy: 0.7820\n",
            "Epoch 6/30\n",
            "46/46 [==============================] - 8s 161ms/step - loss: 0.2605 - accuracy: 0.9077 - val_loss: 0.7430 - val_accuracy: 0.7629\n",
            "Epoch 7/30\n",
            "46/46 [==============================] - 8s 169ms/step - loss: 0.2413 - accuracy: 0.9200 - val_loss: 0.6562 - val_accuracy: 0.7847\n",
            "Epoch 8/30\n",
            "46/46 [==============================] - 8s 171ms/step - loss: 0.1574 - accuracy: 0.9520 - val_loss: 0.6535 - val_accuracy: 0.8038\n",
            "Epoch 9/30\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 0.1386 - accuracy: 0.9530 - val_loss: 0.6382 - val_accuracy: 0.7929\n",
            "Epoch 10/30\n",
            "46/46 [==============================] - 8s 161ms/step - loss: 0.1240 - accuracy: 0.9595 - val_loss: 0.6152 - val_accuracy: 0.7984\n",
            "Epoch 11/30\n",
            "46/46 [==============================] - 8s 163ms/step - loss: 0.1059 - accuracy: 0.9676 - val_loss: 0.7268 - val_accuracy: 0.7847\n",
            "Epoch 12/30\n",
            "46/46 [==============================] - 8s 172ms/step - loss: 0.1124 - accuracy: 0.9690 - val_loss: 0.7204 - val_accuracy: 0.7956\n",
            "Epoch 13/30\n",
            "46/46 [==============================] - 7s 150ms/step - loss: 0.0615 - accuracy: 0.9806 - val_loss: 0.7341 - val_accuracy: 0.8011\n",
            "Epoch 14/30\n",
            "46/46 [==============================] - 8s 174ms/step - loss: 0.0933 - accuracy: 0.9826 - val_loss: 0.7399 - val_accuracy: 0.7929\n",
            "Epoch 15/30\n",
            "46/46 [==============================] - 8s 173ms/step - loss: 0.0538 - accuracy: 0.9871 - val_loss: 0.7325 - val_accuracy: 0.7929\n",
            "Epoch 16/30\n",
            "46/46 [==============================] - 7s 152ms/step - loss: 0.0427 - accuracy: 0.9874 - val_loss: 1.0573 - val_accuracy: 0.7602\n",
            "Epoch 17/30\n",
            "46/46 [==============================] - 7s 160ms/step - loss: 0.0582 - accuracy: 0.9813 - val_loss: 0.8369 - val_accuracy: 0.7929\n",
            "Epoch 18/30\n",
            "46/46 [==============================] - 8s 162ms/step - loss: 0.0540 - accuracy: 0.9884 - val_loss: 0.7858 - val_accuracy: 0.8038\n",
            "Epoch 19/30\n",
            "46/46 [==============================] - 8s 162ms/step - loss: 0.0699 - accuracy: 0.9871 - val_loss: 0.8218 - val_accuracy: 0.8093\n",
            "Epoch 20/30\n",
            "46/46 [==============================] - 7s 161ms/step - loss: 0.0071 - accuracy: 0.9993 - val_loss: 1.5111 - val_accuracy: 0.7248\n",
            "Epoch 21/30\n",
            "46/46 [==============================] - 8s 176ms/step - loss: 0.0651 - accuracy: 0.9881 - val_loss: 0.9046 - val_accuracy: 0.7956\n",
            "Epoch 22/30\n",
            "46/46 [==============================] - 7s 153ms/step - loss: 0.0458 - accuracy: 0.9857 - val_loss: 0.9059 - val_accuracy: 0.7929\n",
            "Epoch 23/30\n",
            "46/46 [==============================] - 8s 162ms/step - loss: 0.0380 - accuracy: 0.9911 - val_loss: 0.9324 - val_accuracy: 0.8147\n",
            "Epoch 24/30\n",
            "46/46 [==============================] - 8s 174ms/step - loss: 0.0487 - accuracy: 0.9905 - val_loss: 0.9651 - val_accuracy: 0.8065\n",
            "Epoch 25/30\n",
            "46/46 [==============================] - 8s 173ms/step - loss: 0.0045 - accuracy: 0.9993 - val_loss: 1.7593 - val_accuracy: 0.7357\n",
            "Epoch 26/30\n",
            "46/46 [==============================] - 8s 162ms/step - loss: 0.0848 - accuracy: 0.9854 - val_loss: 1.0029 - val_accuracy: 0.8147\n",
            "Epoch 27/30\n",
            "46/46 [==============================] - 8s 174ms/step - loss: 0.0411 - accuracy: 0.9928 - val_loss: 0.9730 - val_accuracy: 0.8174\n",
            "Epoch 28/30\n",
            "46/46 [==============================] - 8s 169ms/step - loss: 0.0040 - accuracy: 0.9993 - val_loss: 1.0798 - val_accuracy: 0.7902\n",
            "Epoch 29/30\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 0.0619 - accuracy: 0.9857 - val_loss: 1.0351 - val_accuracy: 0.8038\n",
            "Epoch 30/30\n",
            "46/46 [==============================] - 8s 161ms/step - loss: 0.0464 - accuracy: 0.9898 - val_loss: 1.0814 - val_accuracy: 0.7929\n",
            "Training model with SGD optimizer...\n",
            "Epoch 1/30\n",
            "46/46 [==============================] - 9s 155ms/step - loss: 0.0030 - accuracy: 0.9997 - val_loss: 1.1419 - val_accuracy: 0.8065\n",
            "Epoch 2/30\n",
            "46/46 [==============================] - 7s 161ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 1.1496 - val_accuracy: 0.8038\n",
            "Epoch 3/30\n",
            "46/46 [==============================] - 7s 153ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 1.1545 - val_accuracy: 0.8038\n",
            "Epoch 4/30\n",
            "46/46 [==============================] - 8s 175ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 1.1595 - val_accuracy: 0.8038\n",
            "Epoch 5/30\n",
            "46/46 [==============================] - 8s 164ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 1.1635 - val_accuracy: 0.8038\n",
            "Epoch 6/30\n",
            "46/46 [==============================] - 7s 160ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 1.1670 - val_accuracy: 0.8011\n",
            "Epoch 7/30\n",
            "46/46 [==============================] - 7s 148ms/step - loss: 0.0016 - accuracy: 0.9993 - val_loss: 1.1705 - val_accuracy: 0.8011\n",
            "Epoch 8/30\n",
            "46/46 [==============================] - 8s 161ms/step - loss: 0.0015 - accuracy: 0.9993 - val_loss: 1.1730 - val_accuracy: 0.8011\n",
            "Epoch 9/30\n",
            "46/46 [==============================] - 8s 176ms/step - loss: 0.0015 - accuracy: 0.9993 - val_loss: 1.1753 - val_accuracy: 0.8011\n",
            "Epoch 10/30\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 0.0015 - accuracy: 0.9993 - val_loss: 1.1780 - val_accuracy: 0.8038\n",
            "Epoch 11/30\n",
            "46/46 [==============================] - 8s 163ms/step - loss: 0.0014 - accuracy: 0.9993 - val_loss: 1.1804 - val_accuracy: 0.8038\n",
            "Epoch 12/30\n",
            "46/46 [==============================] - 8s 161ms/step - loss: 0.0014 - accuracy: 0.9993 - val_loss: 1.1823 - val_accuracy: 0.8011\n",
            "Epoch 13/30\n",
            "46/46 [==============================] - 8s 172ms/step - loss: 0.0014 - accuracy: 0.9993 - val_loss: 1.1842 - val_accuracy: 0.8011\n",
            "Epoch 14/30\n",
            "46/46 [==============================] - 8s 163ms/step - loss: 0.0014 - accuracy: 0.9993 - val_loss: 1.1860 - val_accuracy: 0.8011\n",
            "Epoch 15/30\n",
            "46/46 [==============================] - 7s 154ms/step - loss: 0.0014 - accuracy: 0.9993 - val_loss: 1.1875 - val_accuracy: 0.8011\n",
            "Epoch 16/30\n",
            "46/46 [==============================] - 8s 174ms/step - loss: 0.0013 - accuracy: 0.9993 - val_loss: 1.1890 - val_accuracy: 0.8011\n",
            "Epoch 17/30\n",
            "46/46 [==============================] - 8s 165ms/step - loss: 0.0013 - accuracy: 0.9993 - val_loss: 1.1903 - val_accuracy: 0.8011\n",
            "Epoch 18/30\n",
            "46/46 [==============================] - 7s 151ms/step - loss: 0.0013 - accuracy: 0.9993 - val_loss: 1.1915 - val_accuracy: 0.8011\n",
            "Epoch 19/30\n",
            "46/46 [==============================] - 8s 162ms/step - loss: 0.0013 - accuracy: 0.9993 - val_loss: 1.1928 - val_accuracy: 0.8011\n",
            "Epoch 20/30\n",
            "46/46 [==============================] - 8s 173ms/step - loss: 0.0013 - accuracy: 0.9993 - val_loss: 1.1937 - val_accuracy: 0.7984\n",
            "Epoch 21/30\n",
            "46/46 [==============================] - 8s 165ms/step - loss: 0.0013 - accuracy: 0.9993 - val_loss: 1.1949 - val_accuracy: 0.7984\n",
            "Epoch 22/30\n",
            "46/46 [==============================] - 8s 164ms/step - loss: 0.0013 - accuracy: 0.9993 - val_loss: 1.1960 - val_accuracy: 0.7984\n",
            "Epoch 23/30\n",
            "46/46 [==============================] - 8s 162ms/step - loss: 0.0013 - accuracy: 0.9993 - val_loss: 1.1970 - val_accuracy: 0.7984\n",
            "Epoch 24/30\n",
            "46/46 [==============================] - 7s 155ms/step - loss: 0.0013 - accuracy: 0.9993 - val_loss: 1.1981 - val_accuracy: 0.8011\n",
            "Epoch 25/30\n",
            "46/46 [==============================] - 7s 150ms/step - loss: 0.0012 - accuracy: 0.9993 - val_loss: 1.1990 - val_accuracy: 0.8011\n",
            "Epoch 26/30\n",
            "46/46 [==============================] - 8s 170ms/step - loss: 0.0012 - accuracy: 0.9993 - val_loss: 1.1999 - val_accuracy: 0.8011\n",
            "Epoch 27/30\n",
            "46/46 [==============================] - 8s 172ms/step - loss: 0.0012 - accuracy: 0.9993 - val_loss: 1.2009 - val_accuracy: 0.8011\n",
            "Epoch 28/30\n",
            "46/46 [==============================] - 8s 164ms/step - loss: 0.0012 - accuracy: 0.9993 - val_loss: 1.2017 - val_accuracy: 0.8011\n",
            "Epoch 29/30\n",
            "46/46 [==============================] - 8s 164ms/step - loss: 0.0012 - accuracy: 0.9993 - val_loss: 1.2026 - val_accuracy: 0.8011\n",
            "Epoch 30/30\n",
            "46/46 [==============================] - 8s 179ms/step - loss: 0.0012 - accuracy: 0.9993 - val_loss: 1.2036 - val_accuracy: 0.8011\n",
            "Training model with Adadelta optimizer...\n",
            "Epoch 1/30\n",
            "46/46 [==============================] - ETA: 0s - loss: 0.0011 - accuracy: 0.9997"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f09435ef5d90>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Training model with {optimizer_name} optimizer...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mmodel_tfds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     history[optimizer_name] = model_tfds.fit(train_dataset.batch(batch_size), epochs=epochs,\n\u001b[0m\u001b[1;32m     65\u001b[0m                                              validation_data=validation_dataset.batch(batch_size))\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1727\u001b[0m                             \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m                         )\n\u001b[0;32m-> 1729\u001b[0;31m                     val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1730\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m                         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2070\u001b[0m                         ):\n\u001b[1;32m   2071\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adagrad, RMSprop, SGD, Adadelta, Adam\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Load TFDS dataset\n",
        "dataset_name = 'tf_flowers'\n",
        "(train_dataset, validation_dataset, test_dataset), dataset_info = tfds.load(\n",
        "    name=dataset_name,\n",
        "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "    shuffle_files=True,\n",
        "    with_info=True,\n",
        "    as_supervised=True\n",
        ")\n",
        "\n",
        "# Preprocess data\n",
        "def preprocess(image, label):\n",
        "    image = tf.image.resize(image, (150, 150)) / 255.0\n",
        "    return image, label\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess)\n",
        "validation_dataset = validation_dataset.map(preprocess)\n",
        "test_dataset = test_dataset.map(preprocess)\n",
        "\n",
        "# Define model\n",
        "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(150, 150, 3))\n",
        "base_model.trainable = False  # Wagi nie będą trenowane\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "flatten_layer = layers.Flatten()\n",
        "dense_layer_1 = layers.Dense(50, activation='relu')\n",
        "dense_layer_2 = layers.Dense(20, activation='relu')\n",
        "prediction_layer = layers.Dense(5, activation='softmax')\n",
        "\n",
        "model_tfds = models.Sequential([\n",
        "    base_model,\n",
        "    flatten_layer,\n",
        "    dense_layer_1,\n",
        "    dense_layer_2,\n",
        "    prediction_layer\n",
        "])\n",
        "\n",
        "# Define training settings\n",
        "batch_size = 64\n",
        "epochs = 30\n",
        "optimizer_names = ['Adagrad', 'RMSprop', 'SGD', 'Adadelta', 'Adam']\n",
        "optimizers = [Adagrad(), RMSprop(), SGD(), Adadelta(), Adam()]\n",
        "\n",
        "# Train models with different optimizers\n",
        "history = {}\n",
        "for optimizer_name, optimizer in zip(optimizer_names, optimizers):\n",
        "    print(f'Training model with {optimizer_name} optimizer...')\n",
        "    model_tfds.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    history[optimizer_name] = model_tfds.fit(train_dataset.batch(batch_size), epochs=epochs,\n",
        "                                             validation_data=validation_dataset.batch(batch_size))\n",
        "\n",
        "# Plot training loss for all models on one graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "for optimizer_name in optimizer_names:\n",
        "    plt.plot(history[optimizer_name].history['loss'], label=optimizer_name)\n",
        "plt.title('Training Loss for Different Optimizers')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "for optimizer_name in optimizer_names:\n",
        "    plt.plot(history[optimizer_name].history['val_loss'], label=optimizer_name)\n",
        "plt.title('Validation Loss for Different Optimizers')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "for optimizer_name in optimizer_names:\n",
        "    plt.plot(history[optimizer_name].history['accuracy'], label=optimizer_name)\n",
        "plt.title('Accuracy for Different Optimizers')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "for optimizer_name in optimizer_names:\n",
        "    plt.plot(history[optimizer_name].history['val_accuracy'], label=optimizer_name)\n",
        "plt.title('Validation Accuracy for Different Optimizers')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ]
}