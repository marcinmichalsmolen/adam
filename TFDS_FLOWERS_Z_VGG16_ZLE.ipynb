{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMjWPazT/AlcDvoEm2H/Tng",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marcinmichalsmolen/adam/blob/main/TFDS_FLOWERS_Z_VGG16_ZLE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "EW2ZTVJbRuzg",
        "outputId": "de6280b9-4e08-4f9a-8d4d-8ba6886d8796"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-218da202e9f6>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Define model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"imagenet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;31m## Wagi nie będą trenowane\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: '_MapDataset' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adagrad, RMSprop, SGD, Adadelta, Adam\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Load TFDS dataset\n",
        "dataset_name = 'tf_flowers'\n",
        "(train_dataset, validation_dataset, test_dataset), dataset_info = tfds.load(\n",
        "    name=dataset_name,\n",
        "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "    shuffle_files=True,\n",
        "    with_info=True,\n",
        "    as_supervised=True\n",
        ")\n",
        "\n",
        "# Preprocess data\n",
        "def preprocess(image, label):\n",
        "    image = tf.image.resize(image, (150, 150)) / 255.0\n",
        "    return image, label\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess)\n",
        "validation_dataset = validation_dataset.map(preprocess)\n",
        "test_dataset = test_dataset.map(preprocess)\n",
        "\n",
        "# Define model\n",
        "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=train_dataset.shape)\n",
        "base_model.trainable = False ## Wagi nie będą trenowane\n",
        "\n",
        "\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "flatten_layer = layers.Flatten()\n",
        "dense_layer_1 = layers.Dense(50, activation='relu')\n",
        "dense_layer_2 = layers.Dense(20, activation='relu')\n",
        "prediction_layer = layers.Dense(5, activation='softmax')\n",
        "\n",
        "\n",
        "model_tfds = model1 = models.Sequential([\n",
        "    base_model,\n",
        "    flatten_layer,\n",
        "    dense_layer_1,\n",
        "    dense_layer_2,\n",
        "    prediction_layer\n",
        "])\n",
        "\n",
        "\n",
        "# Define training settings\n",
        "batch_size = 64\n",
        "epochs = 30\n",
        "optimizer_names = ['Adagrad', 'RMSprop', 'SGD', 'Adadelta', 'Adam']\n",
        "optimizers = [Adagrad(), RMSprop(), SGD(), Adadelta(), Adam()]\n",
        "\n",
        "# Train models with different optimizers\n",
        "history = {}\n",
        "for optimizer_name, optimizer in zip(optimizer_names, optimizers):\n",
        "    print(f'Training model with {optimizer_name} optimizer...')\n",
        "    model_tfds.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    history[optimizer_name] = model_tfds.fit(train_dataset.batch(batch_size), epochs=epochs,\n",
        "                                             validation_data=validation_dataset.batch(batch_size))\n",
        "\n",
        "# Plot training loss for all models on one graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "for optimizer_name in optimizer_names:\n",
        "    plt.plot(history[optimizer_name].history['loss'], label=optimizer_name)\n",
        "plt.title('Training Loss for Different Optimizers')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "for optimizer_name in optimizer_names:\n",
        "    plt.plot(history[optimizer_name].history['val_loss'], label=optimizer_name)\n",
        "plt.title('Training Loss for Different Optimizers')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Test Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "for optimizer_name in optimizer_names:\n",
        "    plt.plot(history[optimizer_name].history['accuracy'], label=optimizer_name)\n",
        "plt.title('Accuracy for Different Optimizers')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Training Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "for optimizer_name in optimizer_names:\n",
        "    plt.plot(history[optimizer_name].history['val_accuracy'], label=optimizer_name)\n",
        "plt.title('Val_Accuracy for Different Optimizers')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Val_Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ]
}